import { Mensagem } from '../types/conversa';
import openai from '../config/openai';

/**
 * Gera prompt para o c√©rebro da IA com o contexto da conversa
 */
export async function gerarPromptCerebro(
  historico: Mensagem[], 
  mensagemCliente: string
): Promise<string | null> {
  try {
    // Verificar se a API key est√° configurada
    if (!process.env.OPENAI_API_KEY) {
      console.error('‚ùå Erro: OPENAI_API_KEY n√£o est√° configurada');
      return null;
    }

    // Filtra mensagens muito curtas ou vazias
    const historicoFiltrado = historico.filter(msg => msg.texto && msg.texto.length > 2);
    
    // Filtra mensagens muito antigas (mant√©m apenas √∫ltimas 50 mensagens)
    const historicoLimitado = historicoFiltrado.slice(-50);
    
    // Formata o hist√≥rico com timestamps
    const historicoFormatado = historicoLimitado.map((msg) => {
      const timestamp = msg.timestamp ? new Date(msg.timestamp).toLocaleString('pt-BR') : 'Sem timestamp';
      return `[${timestamp}] ${msg.autor === 'usuario' ? 'Cliente' : 'Clara'}: ${msg.texto}`;
    }).join('\n');

    // Prompt simplificado apenas com contexto
    const prompt = `Voc√™ √© Clara, uma assistente virtual da Resoluty Consultoria especializada em ajudar clientes com redu√ß√£o de d√≠vidas banc√°rias.

IMPORTANTE: Voc√™ deve SEMPRE responder diretamente ao cliente de forma natural e amig√°vel. NUNCA mencione "hist√≥rico", "informa√ß√µes insuficientes" ou qualquer termo t√©cnico interno.

Mensagem atual do cliente: ${mensagemCliente}

Baseado na mensagem recebida e no hist√≥rico da conversa, responda de forma natural e direta ao cliente:

- Se o cliente disser seu nome, confirme e pergunte sobre suas d√≠vidas
- Se o cliente mencionar valor de d√≠vida, confirme e ofere√ßa ajuda
- Se o cliente sugerir hor√°rio para reuni√£o, confirme e agende
- Se o cliente disser que recebe sal√°rio em conta, ofere√ßa atendimento personalizado
- Para qualquer outra situa√ß√£o, responda de forma natural e amig√°vel, sempre focando em ajudar com as d√≠vidas

Lembre-se: Responda como se fosse uma conversa natural com o cliente. NUNCA mencione termos t√©cnicos ou internos.

=== HIST√ìRICO DA CONVERSA ===
${historicoFormatado}`;

    console.log('üß† Contexto fornecido (primeiros 500 chars):', prompt.substring(0, 500) + '...');

    // Criar um novo thread para cada conversa
    const thread = await openai.beta.threads.create();
    
    // Adicionar mensagem ao thread
    await openai.beta.threads.messages.create(thread.id, {
      role: 'user',
      content: prompt
    });

    // Executar o assistente com o ID espec√≠fico
    const run = await openai.beta.threads.runs.create(thread.id, {
      assistant_id: 'asst_rPvHoutBw01eSySqhtTK4Iv7'
    });

    // Aguardar conclus√£o
    let runStatus = await openai.beta.threads.runs.retrieve(thread.id, run.id);
    
    while (runStatus.status === 'in_progress' || runStatus.status === 'queued') {
      await new Promise(resolve => setTimeout(resolve, 1000));
      runStatus = await openai.beta.threads.runs.retrieve(thread.id, run.id);
    }

    if (runStatus.status === 'completed') {
      // Buscar a resposta
      const messages = await openai.beta.threads.messages.list(thread.id);
      const lastMessage = messages.data[0];
      
      if (lastMessage && lastMessage.content[0].type === 'text') {
        const aiResponse = lastMessage.content[0].text.value;
        
        // Verificar se a resposta cont√©m termos t√©cnicos que n√£o devem ser mostrados ao cliente
        const termosTecnicos = ['hist√≥rico', 'informa√ß√µes insuficientes', 'dados', 'sistema', 'debug'];
        const contemTermosTecnicos = termosTecnicos.some(termo => 
          aiResponse.toLowerCase().includes(termo.toLowerCase())
        );
        
        if (contemTermosTecnicos) {
          console.log('‚ö†Ô∏è IA retornou resposta com termos t√©cnicos, usando fallback');
          // Fallback para respostas naturais
          if (mensagemCliente.toLowerCase().includes('ol√°') || mensagemCliente.toLowerCase().includes('oi') || mensagemCliente.toLowerCase().includes('ola')) {
            return 'Ol√°! Como posso ajud√°-lo com suas d√≠vidas hoje?';
          }
          if (mensagemCliente.toLowerCase().includes('felipe') || mensagemCliente.toLowerCase().includes('nome')) {
            return 'Ol√° Felipe! Prazer em conhec√™-lo. Como posso ajud√°-lo com suas d√≠vidas banc√°rias?';
          }
          return 'Ol√°! Sou a Clara da Resoluty Consultoria. Como posso ajud√°-lo com suas d√≠vidas hoje?';
        }
        
        console.log('ü§ñ Resposta da IA:', aiResponse);
        return aiResponse;
      } else {
        console.error('‚ùå Erro: IA n√£o retornou resposta v√°lida');
        return null;
      }
    } else {
      console.error('‚ùå Erro na execu√ß√£o da IA:', runStatus.status);
      return null;
    }

  } catch (error) {
    console.error('‚ùå Erro ao gerar resposta da IA:', error);
    
    // Fallback response para evitar quebra do sistema
    if (mensagemCliente.toLowerCase().includes('ol√°') || mensagemCliente.toLowerCase().includes('oi') || mensagemCliente.toLowerCase().includes('ola')) {
      return 'Ol√°! Como posso ajud√°-lo hoje?';
    }
    
    return 'Desculpe, n√£o consegui processar sua mensagem no momento. Um atendente ser√° direcionado para voc√™ em breve.';
  }
}